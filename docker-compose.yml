services:
  training:
    build:
      context: .
      dockerfile: ./docker/training/Dockerfile
    command: tail -f /dev/null
    environment:
      - PYTHONPATH=/harmony/src
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TF_GPU_ALLOCATOR=cuda_malloc_async

      - TF_ENABLE_BF_CONVERSION=1
      # - TF_ENABLE_AUTO_MIXED_PRECISION=1

      - NCCL_LAUNCH_MODE=GROUP
      - NCCL_IB_PCI_RELAXED_ORDERING=1
      - NCCL_DEBUG=WARN

      # - TF_CUDNN_DETERMINISTIC=true # will slow down performance
      - TF_CPP_MIN_LOG_LEVEL=1
    env_file: 
      - path: ./.env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./data/:/volumes/data/
      - ./keras_models/:/volumes/keras_models/
      - ./saved_models/:/volumes/saved_models/
      - ./config/:/volumes/config/
    runtime: nvidia
    restart: no 
    container_name: training 

  storepass_api:
    build: 
      context: .
      dockerfile: docker/storepass_api/Dockerfile
    environment:
      - PYTHONPATH=/harmony/src
      - PYTHONUNBUFFERED=1
      - TF_FORCE_GPU_ALLOW_GROWTH=true
    env_file:
      - path: ./.env
    ports:
      - "${STOREPASS_API_PORT}:5000"
    volumes:
      - ./data/:/volumes/data/
      - ./keras_models/:/volumes/keras_models/
      - ./saved_models/:/volumes/saved_models/
      - ./config/:/volumes/config/
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    restart: unless-stopped
    container_name: storepass_api 

  serving_api:
    build: 
      context: .
      dockerfile: docker/serving_api/Dockerfile
    environment:
      - PYTHONPATH=/harmony/src
      - PYTHONUNBUFFERED=1
      - TF_FORCE_GPU_ALLOW_GROWTH=true
    env_file:
      - path: ./.env
    ports:
      - "${SERVING_API_PORT}:5000"
    volumes:
      - ./data/:/volumes/data/
      - ./keras_models/:/volumes/keras_models/
      - ./saved_models/:/volumes/saved_models/
      - ./config/:/volumes/config/
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    restart: unless-stopped
    container_name: serving_api

  tfs-lorcana:
    image: tensorflow/serving:latest-gpu
    environment: 
      - NVIDIA_VISIBLE_DEVICES=all
      - TF_FORCE_GPU_ALLOW_GROWTH=true
    ports:
      - "${TFS_LORCANA_PORT}:8501"
    volumes:
      - ./saved_models/lorcana/:/models/lorcana/
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command: >
      --model_config_file=/models/lorcana/tfs.config
      --model_config_file_poll_wait_seconds=60
    restart: unless-stopped
    container_name: tfs-lorcana

  tfs-pokemon:
    image: tensorflow/serving:latest-gpu
    environment: 
      - NVIDIA_VISIBLE_DEVICES=all
      - TF_FORCE_GPU_ALLOW_GROWTH=true
    ports:
      - "${TFS_POKEMON_PORT}:8501"
    volumes:
      - ./saved_models/pokemon/:/models/pokemon/
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command: >
      --model_config_file=/models/pokemon/tfs.config
      --model_config_file_poll_wait_seconds=60
    restart: unless-stopped
    container_name: tfs-pokemon

  tfs-yugioh:
    image: tensorflow/serving:latest-gpu
    environment: 
      - NVIDIA_VISIBLE_DEVICES=all
      - TF_FORCE_GPU_ALLOW_GROWTH=true
    ports:
      - "${TFS_YUGIOH_PORT}:8501"
    volumes:
      - ./saved_models/yugioh/:/models/yugioh/
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command: >
      --model_config_file=/models/yugioh/tfs.config
      --model_config_file_poll_wait_seconds=60
    restart: unless-stopped
    container_name: tfs-yugioh

